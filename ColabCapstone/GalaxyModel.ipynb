{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdb1c7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in classification data:\n",
      " GalaxyID     0\n",
      "Class1.1     0\n",
      "Class1.2     0\n",
      "Class1.3     0\n",
      "Class2.1     0\n",
      "Class2.2     0\n",
      "Class3.1     0\n",
      "Class3.2     0\n",
      "Class4.1     0\n",
      "Class4.2     0\n",
      "Class5.1     0\n",
      "Class5.2     0\n",
      "Class5.3     0\n",
      "Class5.4     0\n",
      "Class6.1     0\n",
      "Class6.2     0\n",
      "Class7.1     0\n",
      "Class7.2     0\n",
      "Class7.3     0\n",
      "Class8.1     0\n",
      "Class8.2     0\n",
      "Class8.3     0\n",
      "Class8.4     0\n",
      "Class8.5     0\n",
      "Class8.6     0\n",
      "Class8.7     0\n",
      "Class9.1     0\n",
      "Class9.2     0\n",
      "Class9.3     0\n",
      "Class10.1    0\n",
      "Class10.2    0\n",
      "Class10.3    0\n",
      "Class11.1    0\n",
      "Class11.2    0\n",
      "Class11.3    0\n",
      "Class11.4    0\n",
      "Class11.5    0\n",
      "Class11.6    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "#with zipfile.ZipFile('Datasets/images_training_rev1.zip', 'r') as zip_ref:\n",
    "    # Extract all the files in the zip file to a directory named 'images_training_rev1'\n",
    "    #zip_ref.extractall('Datasets/images_training_rev1')\n",
    "    \n",
    "#with zipfile.ZipFile('Datasets/images_test_rev1.zip', 'r') as zip_ref:\n",
    "    # Extract all the files in the zip file to a directory named 'images_training_rev1'\n",
    "    #zip_ref.extractall('Datasets/images_test_rev1')\n",
    "\n",
    "# Load the CSV file containing the classification data\n",
    "classification_data = pd.read_csv('Datasets/training_solutions_rev1.csv')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = classification_data.isnull().sum()\n",
    "print(\"Missing values in classification data:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a37fdb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in classification data: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = classification_data.duplicated()\n",
    "print(\"Number of duplicate rows in classification data:\", duplicates.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "730164f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of galaxies in each classification category:\n",
      " GalaxyID\n",
      "100008    1\n",
      "100023    1\n",
      "100053    1\n",
      "100078    1\n",
      "100090    1\n",
      "         ..\n",
      "999948    1\n",
      "999950    1\n",
      "999958    1\n",
      "999964    1\n",
      "999967    1\n",
      "Length: 61578, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the balance of the dataset by counting the number of galaxies in each classification category\n",
    "counts = classification_data.groupby('GalaxyID').size()\n",
    "print(\"Number of galaxies in each classification category:\\n\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "714941d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image files: 61578\n"
     ]
    }
   ],
   "source": [
    "# Check the number of image files in the 'images_training_rev1' directory\n",
    "image_files = os.listdir('Datasets/images_training_rev1/images_training_rev1')\n",
    "print(\"Number of image files:\", len(image_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dee3c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image files: 61578\n"
     ]
    }
   ],
   "source": [
    "image_test_files = os.listdir('Datasets/images_test_rev1/images_test_rev1')\n",
    "print(\"Number of image files:\", len(image_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d6c2e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common image and classification IDs: 61578\n"
     ]
    }
   ],
   "source": [
    "image_ids = [int(filename.split('.')[0]) for filename in image_files]\n",
    "classification_ids = classification_data['GalaxyID'].unique()\n",
    "common_ids = set(image_ids).intersection(set(classification_ids))\n",
    "print(\"Number of common image and classification IDs:\", len(common_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d018bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# path = \"Datasets/images_test_rev1_resized\"\n",
    "\n",
    "# # create a dictionary to store the hash values and file paths\n",
    "# hashes = {}\n",
    "\n",
    "# # loop through all the files in the directory\n",
    "# for root, dirs, files in os.walk(path):\n",
    "#     for file in files:\n",
    "#         # open the file and read its contents\n",
    "#         with open(os.path.join(root, file), 'rb') as f:\n",
    "#             data = f.read()\n",
    "#         # compute the hash value of the file's contents\n",
    "#         hash_value = hashlib.md5(data).hexdigest()\n",
    "#         # check if the hash value already exists in the dictionary\n",
    "#         if hash_value in hashes:\n",
    "#             # if it does, print the file path of the duplicate\n",
    "#             print(\"Duplicate found:\", os.path.join(root, file))\n",
    "#         else:\n",
    "#             # if not, add the hash value and file path to the dictionary\n",
    "#             hashes[hash_value] = os.path.join(root, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97d5867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # Define the source and destination directories\n",
    "# src_test_dir = 'Datasets/images_test_rev1/images_test_rev1'\n",
    "# dst_test_dir = 'Datasets/images_test_rev1_resized'\n",
    "\n",
    "# src_train_dir = 'Datasets/images_training_rev1/images_training_rev1'\n",
    "# dst_train_dir = 'Datasets/images_training_rev1_resized'\n",
    "\n",
    "# # Define the target size\n",
    "# target_size = (128, 128)\n",
    "\n",
    "# # Loop over all the images in the source training directory and resize them\n",
    "# for filename in os.listdir(src_train_dir):\n",
    "#     if filename.endswith('.jpg'):\n",
    "#         # Load the image\n",
    "#         img = Image.open(os.path.join(src_train_dir, filename))\n",
    "        \n",
    "#         # Resize the image\n",
    "#         img = img.resize(target_size, Image.ANTIALIAS)\n",
    "        \n",
    "#         # Save the resized image to the destination directory\n",
    "#         img.save(os.path.join(dst_train_dir, filename))\n",
    "\n",
    "# # Loop over all the images in the source test directory and resize them\n",
    "# for filename in os.listdir(src_test_dir):\n",
    "#     if filename.endswith('.jpg'):\n",
    "#         # Load the image\n",
    "#         img = Image.open(os.path.join(src_test_dir, filename))\n",
    "        \n",
    "#         # Resize the image\n",
    "#         img = img.resize(target_size, Image.ANTIALIAS)\n",
    "        \n",
    "#         # Save the resized image to the destination directory\n",
    "#         img.save(os.path.join(dst_test_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "160f84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the dataset and save to disk\n",
    "\n",
    "\n",
    "# # Load the dataset\n",
    "# labeled_data = pd.read_csv('Datasets/training_solutions_rev1.csv')\n",
    "\n",
    "# # Split the dataset into features and labels\n",
    "# X = labeled_data.iloc[:, 1:]\n",
    "# y = labeled_data.iloc[:, :1]\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# # Save the splits to disk\n",
    "# X_train.to_csv('Datasets/train/train_features.csv', index=False)\n",
    "# X_val.to_csv('Datasets//val/val_features.csv', index=False)\n",
    "# X_test.to_csv('Datasets/test/test_features.csv', index=False)\n",
    "# y_train.to_csv('Datasets/train/train_labels.csv', index=False)\n",
    "# y_val.to_csv('Datasets/val/val_labels.csv', index=False)\n",
    "# y_test.to_csv('Datasets/test/test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "020caeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "# set path to the directory containing the image files\n",
    "data_path = \"Datasets/images_training_rev1_resized\"\n",
    "\n",
    "# set path to the training solutions csv file\n",
    "label_path = \"Datasets/training_solutions_rev1.csv\"\n",
    "\n",
    "# set path to the directory where the preprocessed images will be saved\n",
    "save_path = \"Datasets/preprocessed_images\"\n",
    "\n",
    "# set image size\n",
    "img_size = (128, 128)\n",
    "\n",
    "# set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# load the labels\n",
    "labels = pd.read_csv(label_path)\n",
    "\n",
    "# extract the labels as numpy arrays\n",
    "y = labels.iloc[:, 1:].values\n",
    "\n",
    "# function to load and preprocess image\n",
    "def load_image(img_id):\n",
    "    img_filename = os.path.join(data_path, str(img_id) + \".jpg\")\n",
    "    img = cv2.imread(img_filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# generate list of image ids\n",
    "img_ids = labels.iloc[:, 0].values.tolist()\n",
    "\n",
    "# define generator function to load images in batches\n",
    "def image_generator(img_ids, batch_size):\n",
    "    while True:\n",
    "        batch_ids = np.random.choice(img_ids, size=batch_size)\n",
    "        batch_imgs = [load_image(img_id) for img_id in batch_ids]\n",
    "        yield np.array(batch_imgs)\n",
    "\n",
    "# split the dataset into training, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(img_ids, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# convert list of image ids to numpy arrays of preprocessed images\n",
    "X_train = np.array([load_image(img_id) for img_id in X_train])\n",
    "X_val = np.array([load_image(img_id) for img_id in X_val])\n",
    "X_test = np.array([load_image(img_id) for img_id in X_test])\n",
    "\n",
    "# reshape the preprocessed image arrays to have the correct input shape for CNN\n",
    "X_train = X_train.reshape(-1, 128, 128, 3)\n",
    "X_val = X_val.reshape(-1, 128, 128, 3)\n",
    "X_test = X_test.reshape(-1, 128, 128, 3)\n",
    "\n",
    "# save the preprocessed image arrays and labels to disk\n",
    "\n",
    "# save the splits to disk\n",
    "with open(os.path.join(save_path, \"X_train.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(X_train, f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"X_val.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(X_val, f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"X_test.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(X_test, f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"y_train.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(y_train, f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"y_val.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(y_val, f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"y_test.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(y_test, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ed345e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# set path to the directory where the preprocessed images were saved\n",
    "save_path = \"Datasets/preprocessed_images\"\n",
    "\n",
    "# load the preprocessed data from disk\n",
    "with open(os.path.join(save_path, \"X_train.pkl\"), \"rb\") as f:\n",
    "    X_train = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"X_val.pkl\"), \"rb\") as f:\n",
    "    X_val = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"X_test.pkl\"), \"rb\") as f:\n",
    "    X_test = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"y_train.pkl\"), \"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"y_val.pkl\"), \"rb\") as f:\n",
    "    y_val = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(save_path, \"y_test.pkl\"), \"rb\") as f:\n",
    "    y_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "276baf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# define the model architecture\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(37, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6752be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (36946, 128, 128, 3)\n",
      "y_train shape: (36946, 37)\n",
      "X_val shape: (12316, 128, 128, 3)\n",
      "y_val shape: (12316, 37)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08b2bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1155/1155 [==============================] - 408s 353ms/step - loss: 0.2700 - accuracy: 0.6119 - val_loss: 0.2596 - val_accuracy: 0.6559\n",
      "Epoch 2/10\n",
      "1155/1155 [==============================] - 391s 338ms/step - loss: 0.2537 - accuracy: 0.6799 - val_loss: 0.2494 - val_accuracy: 0.7057\n",
      "Epoch 3/10\n",
      "1155/1155 [==============================] - 386s 334ms/step - loss: 0.2467 - accuracy: 0.7250 - val_loss: 0.2467 - val_accuracy: 0.7243\n",
      "Epoch 4/10\n",
      "1155/1155 [==============================] - 385s 334ms/step - loss: 0.2427 - accuracy: 0.7453 - val_loss: 0.2445 - val_accuracy: 0.7405\n",
      "Epoch 5/10\n",
      "1155/1155 [==============================] - 380s 329ms/step - loss: 0.2391 - accuracy: 0.7585 - val_loss: 0.2461 - val_accuracy: 0.7322\n",
      "Epoch 6/10\n",
      "1155/1155 [==============================] - 378s 327ms/step - loss: 0.2358 - accuracy: 0.7731 - val_loss: 0.2482 - val_accuracy: 0.7358\n",
      "Epoch 7/10\n",
      "1155/1155 [==============================] - 380s 329ms/step - loss: 0.2326 - accuracy: 0.7847 - val_loss: 0.2468 - val_accuracy: 0.7273\n",
      "Epoch 8/10\n",
      "1155/1155 [==============================] - 379s 328ms/step - loss: 0.2296 - accuracy: 0.7951 - val_loss: 0.2486 - val_accuracy: 0.7313\n",
      "Epoch 9/10\n",
      "1155/1155 [==============================] - 382s 330ms/step - loss: 0.2269 - accuracy: 0.8068 - val_loss: 0.2493 - val_accuracy: 0.7252\n",
      "Epoch 10/10\n",
      "1155/1155 [==============================] - 377s 327ms/step - loss: 0.2247 - accuracy: 0.8130 - val_loss: 0.2516 - val_accuracy: 0.7096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5a987b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ed950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
